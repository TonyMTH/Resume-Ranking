{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os.path import expanduser, join\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "\n",
    "# from pyrallel.ensemble import EnsembleGrower\n",
    "# from pyrallel.ensemble import sub_ensemble\n",
    "\n",
    "# from ipyparallel import Client\n",
    "# lb_view = Client().load_balanced_view()\n",
    "# len(lb_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/Fold1/train.txt', sep=\" \", header=None)\n",
    "df_test = pd.read_csv('data/Fold1/test.txt', sep=\" \", header=None)\n",
    "df_valid = pd.read_csv('data/Fold1/vali.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>qid:10</td>\n",
       "      <td>1:2</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:0</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:2</td>\n",
       "      <td>6:0.666667</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:0</td>\n",
       "      <td>...</td>\n",
       "      <td>128:1</td>\n",
       "      <td>129:0</td>\n",
       "      <td>130:117</td>\n",
       "      <td>131:55115</td>\n",
       "      <td>132:7</td>\n",
       "      <td>133:2</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>qid:10</td>\n",
       "      <td>1:1</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:1</td>\n",
       "      <td>4:3</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:0.333333</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>128:0</td>\n",
       "      <td>129:0</td>\n",
       "      <td>130:153</td>\n",
       "      <td>131:3866</td>\n",
       "      <td>132:17</td>\n",
       "      <td>133:104</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>qid:10</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:0</td>\n",
       "      <td>129:9</td>\n",
       "      <td>130:266</td>\n",
       "      <td>131:56137</td>\n",
       "      <td>132:5</td>\n",
       "      <td>133:2</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>qid:10</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:2</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>128:8</td>\n",
       "      <td>129:0</td>\n",
       "      <td>130:541</td>\n",
       "      <td>131:12621</td>\n",
       "      <td>132:11</td>\n",
       "      <td>133:11</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>qid:10</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:6</td>\n",
       "      <td>129:0</td>\n",
       "      <td>130:14687</td>\n",
       "      <td>131:40205</td>\n",
       "      <td>132:5</td>\n",
       "      <td>133:3</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1    2    3    4    5    6           7    8           9    ...  \\\n",
       "0    0  qid:10  1:2  2:0  3:0  4:0  5:2  6:0.666667  7:0         8:0  ...   \n",
       "1    0  qid:10  1:1  2:0  3:1  4:3  5:3  6:0.333333  7:0  8:0.333333  ...   \n",
       "2    1  qid:10  1:3  2:0  3:3  4:0  5:3         6:1  7:0         8:1  ...   \n",
       "3    0  qid:10  1:3  2:0  3:2  4:0  5:3         6:1  7:0  8:0.666667  ...   \n",
       "4    1  qid:10  1:3  2:0  3:3  4:0  5:3         6:1  7:0         8:1  ...   \n",
       "\n",
       "     129    130        131        132     133      134    135    136    137  \\\n",
       "0  128:1  129:0    130:117  131:55115   132:7    133:2  134:0  135:0  136:0   \n",
       "1  128:0  129:0    130:153   131:3866  132:17  133:104  134:0  135:0  136:0   \n",
       "2  128:0  129:9    130:266  131:56137   132:5    133:2  134:0  135:0  136:0   \n",
       "3  128:8  129:0    130:541  131:12621  132:11   133:11  134:0  135:0  136:0   \n",
       "4  128:6  129:0  130:14687  131:40205   132:5    133:3  134:0  135:0  136:0   \n",
       "\n",
       "  138  \n",
       "0 NaN  \n",
       "1 NaN  \n",
       "2 NaN  \n",
       "3 NaN  \n",
       "4 NaN  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qid:10'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mslr_dataset_line(line, map_fn_over_feature=None):\n",
    "    \"\"\" \n",
    "    Compact Function to parse a single line from MSLR dataset txt file. \n",
    "\n",
    "    @args:\n",
    "        line: str\n",
    "        map_fn_over_features: fucntion to map over the extracted features\n",
    "\n",
    "    @returns:\n",
    "        Tuple[rel, qid, List[features]]\n",
    "    \"\"\"\n",
    "    # Clean and split into array\n",
    "    tokens = line.strip(\"\\n\").strip(\" \").split(\" \")\n",
    "\n",
    "    # Lambda to parse out the val for qid\n",
    "    extr_fn = lambda x: x.split(\":\")[-1]\n",
    "\n",
    "    if map_fn_over_features is None:\n",
    "        feat_fn = lambda x: str(x)\n",
    "    else:\n",
    "        feat_fn = map_fn_over_features\n",
    "    # one-liner to extract and assign relevance, qid and features\n",
    "    rel, qid, *features = \\\n",
    "    [int(extr_fn(c)) if idx < 2 else feat_fn(c) for idx, c in enumerate(tokens)]\n",
    "\n",
    "    return rel, qid, features\n",
    "\n",
    "import numpy as np\n",
    "def clean(X):\n",
    "    return [float(x.split(\":\")[-1]) if not isinstance(x, int) and not isinstance(x, float) else x for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.apply(lambda x: clean(x))\n",
    "df_train.drop([138],inplace=True,axis=1)\n",
    "\n",
    "df_test = df_test.apply(lambda x: clean(x))\n",
    "df_test.drop([138],inplace=True,axis=1)\n",
    "\n",
    "df_valid = df_valid.apply(lambda x: clean(x))\n",
    "df_valid.drop([138],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = df_train.iloc[1,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[138].isnull().sum(axis = 0) - df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required Columns\n",
    "required_columns = [0,15,20,25,30,35,40,45,75,80,85,90,95,105,110]\n",
    "len(required_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total size in bytes, total number of search results and number of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train.values.nbytes + df_valid.values.nbytes + df_test.values.nbytes) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train.shape[0] + df_valid.shape[0] + df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(df_train.iloc[:,1])) + len(np.unique(df_test.iloc[:,1])) + len(np.unique(df_valid.iloc[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the training and validation sets as a big development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.append(df_valid, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a subset of 500 queries to speed up the learning when prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(df, size, seed=None):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    qid, X, y = df.iloc[:,1].values, df.drop([0,1],axis=1).values, df.iloc[:,0].values\n",
    "    unique_qid = np.unique(qid)\n",
    "    qid_mask = rng.permutation(len(unique_qid))[:size]\n",
    "    subset_mask = np.in1d(qid, unique_qid[qid_mask])\n",
    "    return X[subset_mask], y[subset_mask], qid[subset_mask]\n",
    "\n",
    "\n",
    "X_train_small, y_train_small, qid_train_small = subsample(df, 500, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(qid_train_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_medium, y_train_medium, qid_train_medium = subsample(df, 1000, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_irrelevant(X, y, qid, seed=None):\n",
    "    \"\"\"Subsample the zero-scored entries\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_qid = np.unique(qid)\n",
    "    final_mask = np.ones(shape=y.shape, dtype=bool)\n",
    "    for this_qid in unique_qid:\n",
    "        this_mask = qid == this_qid\n",
    "        this_y = y[this_mask]\n",
    "        relevant = this_y >= 2\n",
    "        ratio = float(np.mean(relevant))\n",
    "        if ratio > 0.5:\n",
    "            # already balanced\n",
    "            continue\n",
    "            \n",
    "        final_mask[this_mask] = np.logical_or(\n",
    "            relevant, np.random.random(len(this_y)) > 0.7) \n",
    "    return X[final_mask], y[final_mask], qid[final_mask]\n",
    "\n",
    "X_balanced_small, y_balanced_small, qid_balanced_small = balance_irrelevant(\n",
    "    X_train_small, y_train_small, qid_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train_small))\n",
    "print(len(y_balanced_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying ranking success with NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances, rank=10):\n",
    "    \"\"\"Discounted cumulative gain at rank (DCG)\"\"\"\n",
    "    relevances = np.asarray(relevances)[:rank]\n",
    "    n_relevances = len(relevances)\n",
    "    if n_relevances == 0:\n",
    "        return 0.\n",
    "\n",
    "    discounts = np.log2(np.arange(n_relevances) + 2)\n",
    "    return np.sum(relevances / discounts)\n",
    " \n",
    " \n",
    "def ndcg(relevances, rank=10):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDGC)\"\"\"\n",
    "    best_dcg = dcg(sorted(relevances, reverse=True), rank)\n",
    "    if best_dcg == 0:\n",
    "        return 0.\n",
    "\n",
    "    return dcg(relevances, rank) / best_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([2, 4, 0, 1, 1, 0, 0], rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([0, 0, 0, 1, 1, 2, 4], rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([0, 0, 0, 1, 1, 2, 4], rank=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([4, 2, 1, 1, 0, 0, 0], rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ndcg(y_true, y_pred, query_ids, rank=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    query_ids = np.asarray(query_ids)\n",
    "    # assume query_ids are sorted\n",
    "    ndcg_scores = []\n",
    "    previous_qid = query_ids[0]\n",
    "    previous_loc = 0\n",
    "    for loc, qid in enumerate(query_ids):\n",
    "        if previous_qid != qid:\n",
    "            chunk = slice(previous_loc, loc)\n",
    "            ranked_relevances = y_true[chunk][np.argsort(y_pred[chunk])[::-1]]\n",
    "            ndcg_scores.append(ndcg(ranked_relevances, rank=rank))\n",
    "            previous_loc = loc\n",
    "        previous_qid = qid\n",
    "\n",
    "    chunk = slice(previous_loc, loc + 1)\n",
    "    ranked_relevances = y_true[chunk][np.argsort(y_pred[chunk])[::-1]]\n",
    "    ndcg_scores.append(ndcg(ranked_relevances, rank=rank))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "\n",
    "mean_ndcg([4, 3, 1, 4, 3], [4, 0, 1, 4, 2], [0, 0, 0, 2, 2], rank=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(model, X, y, qid):\n",
    "    tic = time()\n",
    "    y_predicted = model.predict(X)\n",
    "    prediction_time = time() - tic\n",
    "    print(\"Prediction time: {:.3f}s\".format(prediction_time))\n",
    "    print(\"NDCG@5 score: {:.3f}\".format(\n",
    "    mean_ndcg(y, y_predicted, qid, rank=5)))\n",
    "    print(\"NDCG@10 score: {:.3f}\".format(\n",
    "    mean_ndcg(y, y_predicted, qid, rank=10)))\n",
    "    print(\"NDCG score: {:.3f}\".format(\n",
    "    mean_ndcg(y, y_predicted, qid, rank=None)))\n",
    "    print(\"R2 score: {:.3f}\".format(r2_score(y, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ndcg_by_trees(model, X, y, qid, rank=10):\n",
    "    max_n_trees = len(model.estimators_)\n",
    "    scores = []\n",
    "    \n",
    "    if hasattr(model, 'staged_predict'):\n",
    "        # stage-wise score computation for boosted ensembles\n",
    "        n_trees = np.arange(max_n_trees) + 1\n",
    "        for y_predicted in model.staged_predict(X):\n",
    "            scores.append(mean_ndcg(y, y_predicted, qid, rank=10))\n",
    "    else:\n",
    "        # assume forest-type of tree ensemble: use a log scale to speedup\n",
    "        # the computation\n",
    "        # XXX: partial predictions could be reused\n",
    "        n_trees = np.logspace(0, np.log10(max_n_trees), 10).astype(int)\n",
    "        for j, n in enumerate(n_trees):\n",
    "            y_predicted = ensemble(model, n).predict(X)\n",
    "            scores.append(mean_ndcg(y, y_predicted, qid, rank=rank))\n",
    "            \n",
    "    plt.plot(n_trees, scores)\n",
    "    plt.xlabel(\"Number of trees\")\n",
    "    plt.ylabel(\"Average NDCG@%d\" % rank)\n",
    "    _ = plt.title(\"Impact of the number of trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "etr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[2,3,4,5]\n",
    "t[:None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, qid_test = subsample(df_test, None, seed=0)\n",
    "X_train, y_train, qid_train = subsample(df_train, None, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(etr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(etr, X_train, y_train, qid_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "rfr.fit(X_balanced_small, y_balanced_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(rfr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=200, random_state=1, verbose=1)\n",
    "gbr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(gbr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with a classification to NDCG ranking reduction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_to_relevance(probas):\n",
    "    \"\"\"MCRank-like reduction of classification proba to DCG predictions\"\"\"\n",
    "    rel = np.zeros(probas.shape[0], dtype=np.float32)\n",
    "    for i in range(probas.shape[1]):\n",
    "        rel += i * probas[:, i]\n",
    "    return rel\n",
    "        \n",
    "        \n",
    "class ClassificationRanker(RegressorMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimator_ = clone(self.base_estimator)\n",
    "        self.scaler_ = StandardScaler()\n",
    "        X = self.scaler_.fit_transform(X)\n",
    "        self.estimator_.fit(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        probas = self.estimator_.predict_proba(X_scaled)\n",
    "        return proba_to_relevance(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, random_state=1)\n",
    "gbr = ClassificationRanker(gbc)\n",
    "gbr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(gbr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logr = ClassificationRanker(LogisticRegression(C=1000))\n",
    "logr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(logr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ClassificationRanker(ExtraTreesClassifier(n_estimators=200, random_state=1, n_jobs=-1))\n",
    "etc.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(etc, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6db46579acbdddf7fbf4f4c1844cec333bde4153c6760542894038a6efc1d9c3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('strive': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
